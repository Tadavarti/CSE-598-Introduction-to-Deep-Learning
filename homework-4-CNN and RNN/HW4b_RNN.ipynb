{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW4b",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "(Notebook modified from https://course.ccs.neu.edu/ds4440f20/)\n",
        "\n",
        "# LSTMs and sequence2 sequence models\n",
        "\n",
        "**Instructions:** Answer the questions below in the notebook itself. Submit on canvas your notebook and a pdf printout of your notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EPLQRFFssrp"
      },
      "source": [
        "## Let's work through our exercise: learning to add (with strings)\n",
        "\n",
        "This idea borrowed from the official Keras docs -- I have borrowed some of their data generation code but written PyTorch version. See original version (keras) here: https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py\n",
        "\n",
        "Additional useful links (torch): https://github.com/bentrevett/pytorch-seq2seq\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIh-YgNUCTTU"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsCnTPqRxB6L"
      },
      "source": [
        "class CharacterTable(object):\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one-hot integer representation\n",
        "    + Decode the one-hot or integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "    def __init__(self, chars, one_hot=False):\n",
        "        \"\"\"Initialize character table.\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "        self.one_hot = one_hot\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One-hot encode given string C.\n",
        "        # Arguments\n",
        "            C: string, to be encoded.\n",
        "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        if self.one_hot:\n",
        "          x = np.zeros((num_rows, len(self.chars)))\n",
        "          for i, c in enumerate(C):\n",
        "              x[i, self.char_indices[c]] = 1\n",
        "        else:\n",
        "          x = np.zeros(num_rows)\n",
        "          for i, c in enumerate(C):\n",
        "            x[i] = self.char_indices[c]\n",
        "          \n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"Decode the given vector or 2D array to their character output.\n",
        "        # Arguments\n",
        "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
        "                or a vector of character indices (used with `calc_argmax=False`).\n",
        "            calc_argmax: Whether to find the character index with maximum\n",
        "                probability, defaults to `True`.\n",
        "        \"\"\"\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return ''.join(self.indices_char[x] for x in x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghp5MKg87u-Z"
      },
      "source": [
        "Test the encoding and decoding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbXwn8gg7ERQ",
        "outputId": "1eef3e08-8e44-44ea-b8ca-0b77eadcfed6"
      },
      "source": [
        "import random\n",
        "vocabulary = ['a', 'b', 'c', 'd', 'e']\n",
        "\n",
        "ct = CharacterTable(vocabulary, True)\n",
        "\n",
        "seq = ''.join([random.choice(vocabulary) for i in range(5)])\n",
        "seq_enc = ct.encode(seq, 10)\n",
        "print(seq)\n",
        "print(seq_enc)\n",
        "print(ct.decode(seq_enc))\n",
        "assert seq == ct.decode(seq_enc)[:len(seq)]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "ct = CharacterTable(vocabulary, False)\n",
        "seq_enc = ct.encode(seq, 10)\n",
        "print(seq)\n",
        "print(seq_enc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ddded\n",
            "[[0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "dddedaaaaa\n",
            "================================================================================\n",
            "ddded\n",
            "[3. 3. 3. 4. 3. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9UYEVjGxs_i"
      },
      "source": [
        "Next generate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8jb8dmwxusU",
        "outputId": "d16edfbf-2120-407b-cfa4-4379912cb7e4"
      },
      "source": [
        "# Parameters for the model and dataset.\n",
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
        "# int is DIGITS.\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = '0123456789+ '\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "questions, expected = [], []\n",
        "seen = set()\n",
        "\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
        "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    \n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = '{}+{}'.format(a, b)\n",
        "    query = q + ' ' * (MAXLEN - len(q))\n",
        "    questions.append(query)\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
        "    expected.append(ans)\n",
        "print('Total addition questions:', len(questions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total addition questions: 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBjVaZyXChjX",
        "outputId": "b8e1f63f-13f0-4c76-d428-f95e39170295"
      },
      "source": [
        "# e.g.\n",
        "print(list(zip(questions[:5], expected[:5])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('8+994  ', '1002'), ('838+732', '1570'), ('70+569 ', '639 '), ('759+162', '921 '), ('32+704 ', '736 ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBmry7Fxybag"
      },
      "source": [
        "Now vectorize, split into train and val data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsHm9_TAybl6",
        "outputId": "07321433-ea8f-4268-ebab-53e9821484d2"
      },
      "source": [
        "print('Vectorization...')\n",
        "# note that the pytorch (nn) Embedding layer wants indices as inputs (not one-hot)\n",
        "x = np.zeros((len(questions), MAXLEN), dtype=int) # len(chars)), dtype=int)\n",
        "y = np.zeros((len(questions), DIGITS + 1), dtype=int) #, len(chars)), dtype=int)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "    \n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "\n",
        "x[0].shape # MAXLEN x num chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorization...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(7,)"
            ]
          },
          "execution_count": 17,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYw9xyCojAR3",
        "outputId": "c4253359-0bd8-4423-fe58-95700b571daa"
      },
      "source": [
        "x[0,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([10,  1, 11, 11,  6,  0,  0])"
            ]
          },
          "execution_count": 18,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bSPok_t7w1_",
        "outputId": "b003bc78-20ce-4bd2-d01b-bf4c7960034f"
      },
      "source": [
        "y[0].shape # (DIGITS + 1) x num chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "execution_count": 19,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALnBQn1yFoSn",
        "outputId": "52a62a17-1d3c-4df6-8ecc-f8050e124903"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3, 2, 2, 4])"
            ]
          },
          "execution_count": 20,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAOeZgapyzLA"
      },
      "source": [
        "Shuffle, split into train/val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r93RRiX6yg3P",
        "outputId": "0d564316-9c0b-404b-9ef2-f93969ac4e4a"
      },
      "source": [
        "# Shuffle (x, y) in unison\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = torch.from_numpy(x[indices])\n",
        "y = torch.from_numpy(y[indices])\n",
        "\n",
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print('Training Data:')\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print()\n",
        "print('Validation Data:')\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data:\n",
            "torch.Size([45000, 7])\n",
            "torch.Size([45000, 4])\n",
            "\n",
            "Validation Data:\n",
            "torch.Size([5000, 7])\n",
            "torch.Size([5000, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA7Bhp-t8r3w"
      },
      "source": [
        "Now let's define our encoder/decoder model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMRjjZVODUmo",
        "outputId": "35e10962-5639-4683-81e3-dfb048c3a951"
      },
      "source": [
        "x[0,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([4, 2, 3, 1, 6, 9, 2])"
            ]
          },
          "execution_count": 22,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohdmnc8p9So_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_dim, emb_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.input_dim = input_dim\n",
        "    self.embed_dim = emb_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    \n",
        "    self.embed_layer = nn.Embedding(self.input_dim, self.embed_dim)\n",
        "    # batch_first --> (batch, seq, feature)\n",
        "    self.rnn = nn.LSTM(self.embed_dim, self.hidden_dim, batch_first=True)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # x_e = (batch x length x dims)\n",
        "    x_e = self.embed_layer(x)\n",
        "    outputs, h = self.rnn(x_e)\n",
        "    return h\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLJYxnI6U1PB"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_dim, embed_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embed_dim = embed_dim\n",
        "    self.output_dim = output_dim\n",
        "    \n",
        "    self.embed_layer = nn.Embedding(self.input_dim, self.embed_dim)\n",
        "    # batch_first --> (batch, seq, feature)\n",
        "    self.rnn = nn.LSTM(self.embed_dim, self.hidden_dim, batch_first=True)\n",
        "    self.out = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "    self.sm = nn.Softmax(dim=-1)\n",
        "    \n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "    x_e = self.embed_layer(x)\n",
        "    output, (h, c) = self.rnn(x_e, hidden)\n",
        "    out = self.out(h)\n",
        "    \n",
        "    y_hat = self.sm(out)\n",
        "  \n",
        "    return y_hat, (h, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkwQ3xPjCKC3"
      },
      "source": [
        "**Q1. Answer the questions below:**\n",
        "\n",
        "Read about the Encoder-Decoder Architecture here: https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html\n",
        "\n",
        "- In this particular problem, what are the elements of the input sequence (list all possible values)?\n",
        "- In this particular problem, what are the elements of the output sequence (list all possible values)?\n",
        "- In this particular problem, which sequence does the Encoder encode?\n",
        "- Which state of the LSTM does the Encoder use as the encoding of the input sequence (recall that the LSTM calculates a state after each element of the input sequence)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuxVM4d8oi16"
      },
      "source": [
        "- [0,1,2,3,4,5,6,7,8,9,+,' '] are the possible values of input sequence.\n",
        "\n",
        "- Similar to input sequence output sequence is represented with one of the numbers between 0-9 and ' ' with max length of 3.\n",
        "\n",
        "- The encoder encodes the string sequence of adding two numbers, containing a maxing 3 digit integers.\n",
        "\n",
        "- To Encode Input Sequence, encoder uses Hidden State of the LSTM. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qls313WkH-ZE"
      },
      "source": [
        "import random \n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "       \n",
        "    def forward(self, x, y, teacher_forcing_ratio=0.5):\n",
        "        \n",
        "        # src = [src sent len, batch size]\n",
        "        # trg = [trg sent len, batch size]\n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = y.shape[0]\n",
        "        max_len = y.shape[1]\n",
        "        vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        # tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_len, batch_size, vocab_size).to(\"cuda\")\n",
        "        \n",
        "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden = self.encoder(x)\n",
        "        \n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        y_hat = torch.zeros(batch_size, dtype=torch.long).unsqueeze(1)\n",
        "        for t in range(0, max_len):\n",
        " \n",
        "            output, hidden = self.decoder(y_hat, hidden)          \n",
        "            outputs[t] = output\n",
        "    \n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            # the 2 arg is the dimension to reduce; skips batch and\n",
        "            # and pulls max index from each softmax (for each instance)\n",
        "            max_preds = output.max(2)[1] # will be 1 x batch\n",
        "            # tranpose to batch x 1\n",
        "            y_hat = max_preds.transpose(0,1)\n",
        "            if teacher_force:\n",
        "              # then replace predictions with the reference\n",
        "              y_hat = y[:,t].unsqueeze(1)\n",
        "\n",
        "        \n",
        "        # need to flip dims around to be \n",
        "        # (batch x length x vocab)\n",
        "        return outputs.permute(1, 0, 2)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9tfuwxnLFQM",
        "outputId": "ef5904ac-b82b-416b-d345-549cdd536335"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([8, 9, 3, 0])"
            ]
          },
          "execution_count": 26,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNrafjnrJSzn"
      },
      "source": [
        "embed_dim = 32\n",
        "hidden_dim = 32\n",
        "vocab_size = len(chars)\n",
        "\n",
        "encoder = Encoder(vocab_size, embed_dim, hidden_dim)\n",
        "decoder = Decoder(vocab_size, embed_dim, hidden_dim, vocab_size)\n",
        "\n",
        "s2s = Seq2Seq(encoder, decoder)\n",
        "\n",
        "output = s2s(x[:8], y[:8])\n",
        "\n",
        "output.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "2DDzVRZ8jeJu",
        "outputId": "cdddd2a1-e32b-4ad8-94f1-a204de5fb7e5"
      },
      "source": [
        "y_target = y[:8]\n",
        "y_target.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-21d26542cb88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4jEga8HqR4s",
        "outputId": "ede9fbbe-9cfd-4b26-dfda-ce070327fc89"
      },
      "source": [
        "y_target[0,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([4, 8, 2, 0])"
            ]
          },
          "execution_count": 153,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5PEzxOpqZGG",
        "outputId": "6fa5b343-b60a-424a-acd9-d08fdcf2f79b"
      },
      "source": [
        "output[0,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0838, 0.0752, 0.1075, 0.0687, 0.0741, 0.0896, 0.0731, 0.0947, 0.0818,\n",
              "         0.0623, 0.1076, 0.0816],\n",
              "        [0.0864, 0.0861, 0.0970, 0.0689, 0.0701, 0.0921, 0.0811, 0.0873, 0.0770,\n",
              "         0.0716, 0.1010, 0.0814],\n",
              "        [0.0776, 0.0818, 0.1048, 0.0703, 0.0719, 0.0899, 0.0750, 0.0838, 0.0835,\n",
              "         0.0767, 0.0969, 0.0878],\n",
              "        [0.0866, 0.0818, 0.1047, 0.0748, 0.0678, 0.0830, 0.0707, 0.0825, 0.0736,\n",
              "         0.0781, 0.1065, 0.0900]], device='cuda:0', grad_fn=<SliceBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16XkEb5WTp-X",
        "outputId": "1474a18f-6895-4025-d545-f5f65d3c24bf"
      },
      "source": [
        "y_target.flatten().shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32])"
            ]
          },
          "execution_count": 155,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDLURUeJUuF9"
      },
      "source": [
        "C = len(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gji8Xz_RlErb"
      },
      "source": [
        "# need to create (N x C) so collapse first two dims\n",
        "output = output.contiguous().view(-1, C)\n",
        "# this is just going to be (B x 1) -- B being batch size\n",
        "y_target = y_target.flatten().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdXfuhJlSSM4",
        "outputId": "5e87d879-1c71-4f40-b6c8-cc7e0879d8f6"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 12])"
            ]
          },
          "execution_count": 158,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCi0wYgPtfRa"
      },
      "source": [
        "from torch import optim\n",
        "optimizer = optim.Adam(s2s.parameters())\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tFQc1ldKYCp",
        "outputId": "b245c020-b50f-4e5d-a4f9-18f6b8edd9d5"
      },
      "source": [
        "loss = criterion(output, y_target)\n",
        "print(loss.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-0.08508788049221039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YdYBVkGiOmu"
      },
      "source": [
        "Now, train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abfRTGIxh-Jg"
      },
      "source": [
        "def train(model, x, y, optimizer, criterion, batch_size=16, epochs=10):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i in range(epochs):\n",
        "      current_idx = 0 \n",
        "   \n",
        "      while (current_idx + batch_size) < x.shape[0]:\n",
        "        optimizer.zero_grad()\n",
        "        batch_x, batch_y = x[current_idx:current_idx+batch_size], y[current_idx:current_idx+batch_size]\n",
        "        \n",
        "        output = model(batch_x, batch_y)\n",
        "        \n",
        "        # flatten\n",
        "        #output_flat = output.contiguous().view(-1, vocab_size)\n",
        "        output_flat = output.contiguous().view(-1, vocab_size)\n",
        "        y_flat = batch_y.view(-1).cuda()\n",
        "       \n",
        "        loss = criterion(output_flat, y_flat)\n",
        "       \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        current_idx += batch_size\n",
        "        \n",
        "      print(f\"epoch {i} loss: {epoch_loss:.2f}\")\n",
        "    return model\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNtacHTItPbp",
        "outputId": "c32d03d7-59f9-4ce8-9b9c-525debfc35d8"
      },
      "source": [
        "mm = train(s2s, x_train, y_train, optimizer, criterion, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 loss: -919.32\n",
            "epoch 1 loss: -1870.89\n",
            "epoch 2 loss: -2825.68\n",
            "epoch 3 loss: -3782.19\n",
            "epoch 4 loss: -4739.48\n",
            "epoch 5 loss: -5697.45\n",
            "epoch 6 loss: -6655.45\n",
            "epoch 7 loss: -7613.74\n",
            "epoch 8 loss: -8572.94\n",
            "epoch 9 loss: -9532.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e16QzkRvIcW3"
      },
      "source": [
        "Make predictions for first 10 instances in validation set. We are passing in y_val (which we would not actually have in practice!) just for convienence -- note that we set the teacher forcing ratio to 0, and hence this is not used during decoding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6FwlsJVVW7AS",
        "outputId": "6d0c27cf-7fd6-443a-94db-69343149e9d2"
      },
      "source": [
        "predictions = s2s(x_train[:10], y_train[:10])\n",
        "predictions = predictions.cpu().detach().numpy()\n",
        "print(predictions.shape)\n",
        "ctable.decode(predictions[3,:,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10, 4, 12)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'111 '"
            ]
          },
          "execution_count": 171,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "e28JNNfLDJWo",
        "outputId": "612b8f2c-c2a8-42d4-ec18-708a060de25c"
      },
      "source": [
        "y_train0 = y_train[0].cpu().detach().numpy()\n",
        "ctable.decode(y_train0, calc_argmax=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'260 '"
            ]
          },
          "execution_count": 172,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvBvK1UjDGbL"
      },
      "source": [
        "predictions = s2s(x_val[:10], y_val[:10], teacher_forcing_ratio=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSbanqmDInye",
        "outputId": "d70efc77-1542-45cb-a35d-24b787022d69"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 4, 12])"
            ]
          },
          "execution_count": 174,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uJBFIdHJRH5"
      },
      "source": [
        "predictions = predictions.cpu().detach().numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iPIBoudJhlE",
        "outputId": "b914438c-5bf7-457f-8f88-b76dd1efd028"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10, 4, 12)"
            ]
          },
          "execution_count": 176,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oI_s4L7MJ-Ub",
        "outputId": "a8a933a2-10e5-45bc-bb1c-7bc30189e0ff"
      },
      "source": [
        "x_val0 = x_val[1].cpu().detach().numpy()\n",
        "x_val0.shape\n",
        "ctable.decode(x_val0, calc_argmax=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'69+937 '"
            ]
          },
          "execution_count": 177,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fhds2ag3Ix-J",
        "outputId": "ddc36c96-94b4-44ac-856d-aa8372f63201"
      },
      "source": [
        "ctable.decode(predictions[0,:,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'111 '"
            ]
          },
          "execution_count": 178,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztl-lP2nPSpx",
        "outputId": "6d278d8c-d7d2-4ada-a85e-1603b43ec4dc"
      },
      "source": [
        "predictions[1,0,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([9.6621365e-12, 1.7318649e-10, 9.0345847e-10, 1.0000000e+00,\n",
              "       1.2083294e-09, 9.2383262e-10, 1.0086231e-09, 8.0573348e-10,\n",
              "       1.9271300e-09, 8.5161683e-10, 1.4581296e-09, 5.4421723e-10],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 179,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "r0Ia172bK_EZ",
        "outputId": "70ebf508-f75c-4850-c65d-94698bcf2c23"
      },
      "source": [
        "y_val0 = y_val[1].cpu().detach().numpy()\n",
        "ctable.decode(y_val0, calc_argmax=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1006'"
            ]
          },
          "execution_count": 180,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iijr_wqcF4-G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}